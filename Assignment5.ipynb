{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "\n",
    "\n",
    "*Please fill out the relevant cells below according to the instructions. When done, save the notebook and export it to PDF, upload both the `ipynb` and the PDF file to Canvas.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members\n",
    "\n",
    "*Group submission is highly encouraged. If you submit as part of group, list all group members here. Groups can comprise up to 4 students.*\n",
    "\n",
    "* Adam Applegate\n",
    "* Beatrix Brahms\n",
    "* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Automatic Hubble (3pts)\n",
    "\n",
    "Go back to Assignment 3. Get the Hubble data and take the GP error model from Problem 1.3 to solve the linear regression with correlated errors, but this time use `emcee` to get the posteriors.\n",
    "\n",
    "Adopt non-informative priors, i.e. $p(b) \\propto (1+b^2)^{-3/2}$ and $p(\\sigma_y)\\propto 1/\\sigma_y$, and whatever you feel is appropriate for the priors of the parameter $\\alpha$ and $\\tau$ of the Matern kernel.\n",
    "\n",
    "Plot the parameter contours with `corner` and make another plot that compares the data with samples of the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: HMC for Hierarchical Regression (7pts)\n",
    "\n",
    "Implement the hierarchical linear regression model for math scores vs socio-econonic status (SES) from 100 schools we discussed in the lecture. But instead of the complicated Gibbs sampling sequence with its full conditionals, compute posteriors with HMC/NUTS.\n",
    "\n",
    "## Problem 2.1 (3pts):\n",
    "\n",
    "Get the data from the Hoff 2009 book [here](https://github.com/probml/pmtk3/blob/master/data/mathDataHoff.csv).\n",
    "Treat each school independently and perform linear regression (with an unknown error) of math score as function of SES. Specifically, implement this model in `numpyro`:\n",
    "\n",
    "\\begin{align}\n",
    "\\mu &\\sim \\mathcal{N}(\\mu_0, V_0)\\\\\n",
    "\\Sigma &\\sim \\mathrm{IW} (\\eta_0, S_0^{-1})\\\\\n",
    "w_j&\\sim\\mathcal{N}(\\mu,\\Sigma)\\\\\n",
    "\\sigma^2 &\\sim \\mathrm{IG} (\\tfrac{1}{2}\\nu_0, \\tfrac{1}{2}\\nu_0 \\sigma_0^2)\\\\\n",
    "e_{ij}&\\sim\\mathcal{N}(0, \\sigma^2)\n",
    "\\end{align}\n",
    "\n",
    "Take the mean of the posterior samples for the per-school linear regression weights and make a plot of these mean posterior predictions (see panel a) of Murphy Fig. 24.4). Store the posterior means of $w_j$ and $\\sigma_j$ for every school for Problem 2.3.\n",
    "\n",
    "**Hints**: \n",
    "\n",
    "* Pick reasonable values for the hyper-parameters $\\mu_0, V_0,\\dots$. A good method is to plot the resulting parameter distributions and compare them to the data. They should be wide enough to not overly influence the inference.\n",
    "* The inverse Wishart distribution is not available in NumPyro yet. For univariate cases, it is identical to the inverse Gamma distribution. For the $2\\times 2$ covariance $\\Sigma$, assume independence of the slope and the intercept variances. Thus, sample two independent RVs from the inverse Gamma and treat them like a diagonal matrix covariance matrix. \n",
    "* ```\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2 (3pts):\n",
    "\n",
    "Now implement the full hierarchical scheme for the school regression coefficient $w_j$ we introduced in the lecture. Use the same hyperprior parameterization you used in 2.1. In other words, the only thing that changes is the linear model now predicts the values for the entire data set. \n",
    "\n",
    "Make a corner plot of the posterior for the set of hyperparameters $(\\mu, \\Sigma, \\sigma)$. In another plot, show the linear regression from the mean posterior prediction for every school. Store the posterior means of $w_j$ for every school and of the global $\\sigma$ for Problem 2.3.\n",
    "\n",
    "**Hint:** `numpyro.plate` could be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3 (1pt):\n",
    "\n",
    "Make a [quiver plot](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.quiver.html) with the coordinates $(w_{j,0}, \\sigma_j)$ from 2.1 as starting points and $(w_{j,0},\\sigma)$ from 2.2 as end points (one arrow per school). Do the same with $w_{j,1}$ instead of $w_{j,0}$. Color-code each arrow with the number of students in each school. Interpret the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
